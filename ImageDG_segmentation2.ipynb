{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekbansal1977/ML/blob/main/ImageDG_segmentation2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "p10WQAAGOgzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQTTEXr-CIdK"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow tensorflow_addons tensorflow_datasets tensorflow_hub numpy matplotlib seaborn sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "dTqFUkuHCV-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the whole dataset, for data info\n",
        "all_ds   = tfds.load(\"eurosat\", with_info=True)\n",
        "# load training, testing & validation sets, splitting by 60%, 20% and 20% respectively\n",
        "train_ds = tfds.load(\"eurosat\", split=\"train[:60%]\")\n",
        "test_ds  = tfds.load(\"eurosat\", split=\"train[60%:80%]\")\n",
        "valid_ds = tfds.load(\"eurosat\", split=\"train[80%:]\")"
      ],
      "metadata": {
        "id": "MOPUL0INCgox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the class names\n",
        "class_names = all_ds[1].features[\"label\"].names\n",
        "# total number of classes (10)\n",
        "num_classes = len(class_names)\n",
        "num_examples = all_ds[1].splits[\"train\"].num_examples"
      ],
      "metadata": {
        "id": "WltyToEHC81B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a plot for number of samples on each class\n",
        "fig, ax = plt.subplots(1, 1, figsize=(14,10))\n",
        "labels, counts = np.unique(np.fromiter(all_ds[0][\"train\"].map(lambda x: x[\"label\"]), np.int32), \n",
        "                       return_counts=True)\n",
        "\n",
        "plt.ylabel('Counts')\n",
        "plt.xlabel('Labels')\n",
        "sns.barplot(x = [class_names[l] for l in labels], y = counts, ax=ax) \n",
        "for i, x_ in enumerate(labels):\n",
        "  ax.text(x_-0.2, counts[i]+5, counts[i])\n",
        "# set the title\n",
        "ax.set_title(\"Bar Plot showing Number of Samples on Each Class\")\n",
        "# save the image\n",
        "# plt.savefig(\"class_samples.png\")"
      ],
      "metadata": {
        "id": "ZlALIRzmDCMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_for_training(ds, cache=True, batch_size=64, shuffle_buffer_size=1000):\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "  ds = ds.map(lambda d: (d[\"image\"], tf.one_hot(d[\"label\"], num_classes)))\n",
        "  # shuffle the dataset\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "  # Repeat forever\n",
        "  ds = ds.repeat()\n",
        "  # split to batches\n",
        "  ds = ds.batch(batch_size)\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "  # is training.\n",
        "  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "aZZgr9buDjP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# preprocess training & validation sets\n",
        "train_ds = prepare_for_training(train_ds, batch_size=batch_size)\n",
        "valid_ds = prepare_for_training(valid_ds, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "eeZ6OSVUDnJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validating shapes\n",
        "for el in valid_ds.take(1):\n",
        "  print(el[0].shape, el[1].shape)\n",
        "for el in train_ds.take(1):\n",
        "  print(el[0].shape, el[1].shape)"
      ],
      "metadata": {
        "id": "9R3FXQQ9DtK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take the first batch of the training set\n",
        "batch = next(iter(train_ds))"
      ],
      "metadata": {
        "id": "l1tAZSXbDw3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_batch(batch):\n",
        "  plt.figure(figsize=(16, 16))\n",
        "  for n in range(min(32, batch_size)):\n",
        "      ax = plt.subplot(batch_size//8, 8, n + 1)\n",
        "      # show the image\n",
        "      plt.imshow(batch[0][n])\n",
        "      # and put the corresponding label as title upper to the image\n",
        "      plt.title(class_names[tf.argmax(batch[1][n].numpy())])\n",
        "      plt.axis('off')\n",
        "      plt.savefig(\"sample-images.png\")\n",
        "\n",
        "# showing a batch of images along with labels\n",
        "show_batch(batch)"
      ],
      "metadata": {
        "id": "RZ9foTF0DyWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\"\n",
        "\n",
        "# download & load the layer as a feature vector\n",
        "keras_layer = hub.KerasLayer(model_url, output_shape=[1280], trainable=True)\n"
      ],
      "metadata": {
        "id": "0qCtSm3jD7Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = tf.keras.Sequential([\n",
        "  keras_layer,\n",
        "  tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "# build the model with input image shape as (64, 64, 3)\n",
        "m.build([None, 64, 64, 3])\n",
        "m.compile(\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    optimizer=\"adam\", \n",
        "    metrics=[\"accuracy\", tfa.metrics.F1Score(num_classes)]\n",
        ")"
      ],
      "metadata": {
        "id": "tZT46VofEKFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.summary()"
      ],
      "metadata": {
        "id": "dObOWtxJEQwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"satellite-classification\"\n",
        "model_path = os.path.join(\"results\", model_name + \".h5\")\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "IElNXGUsEUZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training & validation steps since we're using .repeat() on our dataset\n",
        "# number of training steps\n",
        "n_training_steps   = int(num_examples * 0.6) // batch_size\n",
        "# number of validation steps\n",
        "n_validation_steps = int(num_examples * 0.2) // batch_size"
      ],
      "metadata": {
        "id": "hD8eQG-QEdSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "history = m.fit(\n",
        "    train_ds, validation_data=valid_ds,\n",
        "    steps_per_epoch=n_training_steps, \n",
        "    validation_steps=n_validation_steps,\n",
        "    verbose=1, epochs=5, \n",
        "    callbacks=[model_checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "91fFhKiPEigf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best weights\n",
        "m.load_weights(model_path)"
      ],
      "metadata": {
        "id": "pgN3mjY3FYNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of testing steps\n",
        "n_testing_steps = int(all_ds[1].splits[\"train\"].num_examples * 0.2)\n",
        "# get all testing images as NumPy array\n",
        "images = np.array([ d[\"image\"] for d in test_ds.take(n_testing_steps) ])\n",
        "print(\"images.shape:\", images.shape)\n",
        "# get all testing labels as NumPy array\n",
        "labels = np.array([ d[\"label\"] for d in test_ds.take(n_testing_steps) ])\n",
        "print(\"labels.shape:\", labels.shape)"
      ],
      "metadata": {
        "id": "cCkUHNq-IuAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feed the images to get predictions\n",
        "predictions = m.predict(images)\n",
        "# perform argmax to get class index\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "print(\"predictions.shape:\", predictions.shape)"
      ],
      "metadata": {
        "id": "hl1-R7RZIwvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "accuracy.update_state(labels, predictions)\n",
        "print(\"Accuracy:\", accuracy.result().numpy())\n",
        "print(\"F1 Score:\", f1_score(labels, predictions, average=\"macro\"))"
      ],
      "metadata": {
        "id": "N9py_Jz8IzaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the confusion matrix\n",
        "cmn = tf.math.confusion_matrix(labels, predictions).numpy()\n",
        "# normalize the matrix to be in percentages\n",
        "cmn = cmn.astype('float') / cmn.sum(axis=0)[:, np.newaxis]\n",
        "# make a plot for the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(cmn, annot=True, fmt='.2f', \n",
        "            xticklabels=[f\"pred_{c}\" for c in class_names], \n",
        "            yticklabels=[f\"true_{c}\" for c in class_names],\n",
        "            # cmap=\"Blues\"\n",
        "            cmap=\"rocket_r\"\n",
        "            )\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "# plot the resulting confusion matrix\n",
        "plt.savefig(\"confusion-matrix.png\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SOoj1IpTI15S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_predicted_samples():\n",
        "  plt.figure(figsize=(14, 14))\n",
        "  for n in range(64):\n",
        "      ax = plt.subplot(8, 8, n + 1)\n",
        "      # show the image\n",
        "      plt.imshow(images[n])\n",
        "      # and put the corresponding label as title upper to the image\n",
        "      if predictions[n] == labels[n]:\n",
        "        # correct prediction\n",
        "        ax.set_title(class_names[predictions[n]], color=\"green\")\n",
        "      else:\n",
        "        # wrong prediction\n",
        "        ax.set_title(f\"{class_names[predictions[n]]}/T:{class_names[labels[n]]}\", color=\"red\")\n",
        "      plt.axis('off')\n",
        "      plt.savefig(\"predicted-sample-images.png\")\n",
        "\n",
        "# showing a batch of images along with predictions labels\n",
        "show_predicted_samples()"
      ],
      "metadata": {
        "id": "HnINzxtTI6Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4AvcPm22rzJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "N72IT3cjI94C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}